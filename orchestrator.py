import asyncio
import json
from pathlib import Path
from app.services.auth_service import reddit_login
from app.services.scrape_service import scrape_user_profile, scrape_subreddit_about
from app.services.storage_service import upsert_username_list, upsert_subreddit_info
from app.utils.logger import logger
from app.db.database import init_db
from app.ai_client import generate_message
from app.utils.proxy_manager import get_next_working_proxy, proxies_list
import itertools
import httpx

BASE_DIR = Path(__file__).resolve().parent
CONFIG_FILE = BASE_DIR / "config.json"

USERNAME = "daniellikescoffee123"
PASSWORD = "RNeixv617fjv6nJ*Yfc+q3k!3R"

async def login_and_get_cookies(proxy=None):
    """Face login È™i returneazÄƒ cookies + headers, cu sau fÄƒrÄƒ proxy."""
    if proxy:
        logger.info(f"ğŸŒ Login folosind proxy: {proxy}")
        async with httpx.AsyncClient(proxy=proxy, timeout=30) as base_session:
            logged_in, _ = await reddit_login(USERNAME, PASSWORD, session=base_session)
            if not logged_in:
                return None, None
            cookies = base_session.cookies.jar
            headers = base_session.headers.copy()
        return cookies, headers
    else:
        logger.info("ğŸŒ Login fÄƒrÄƒ proxy")
        async with httpx.AsyncClient(timeout=30) as base_session:
            logged_in, _ = await reddit_login(USERNAME, PASSWORD, session=base_session)
            if not logged_in:
                return None, None
            cookies = base_session.cookies.jar
            headers = base_session.headers.copy()
        return cookies, headers

async def create_sessions_with_proxies(cookies, headers):
    """
    CreeazÄƒ cÃ¢te o sesiune httpx pentru fiecare proxy funcÈ›ional.
    DacÄƒ niciun proxy nu e funcÈ›ional, face fallback la o singurÄƒ sesiune fÄƒrÄƒ proxy.
    ReturneazÄƒ Ã®ntotdeauna un itertools.cycle cu cel puÈ›in o sesiune.
    """
    sessions = []

    # Ã®ncearcÄƒ fiecare proxy din listÄƒ; pÄƒstreazÄƒ doar cele funcÈ›ionale
    for proxy in proxies_list:
        # folosim testerul tÄƒu; dacÄƒ nu e funcÈ›ional, Ã®l sÄƒrim
        try:
            working = await get_next_working_proxy(start_proxy=proxy)
        except TypeError:
            # dacÄƒ signÄƒtura funcÈ›iei tale e fÄƒrÄƒ start_proxy, foloseÈ™te rotaÈ›ia internÄƒ
            working = await get_next_working_proxy()

        if not working:
            logger.warning(f"âš ï¸ Proxy {proxy} nu este funcÈ›ional, Ã®l sar.")
            continue

        client = httpx.AsyncClient(proxy=working, timeout=30)
        client.cookies.update(cookies)
        client.headers.update(headers)
        setattr(client, "proxy_url", working)
        sessions.append(client)

    if not sessions:
        logger.warning("âš ï¸ Niciun proxy funcÈ›ional â€” folosesc o sesiune fÄƒrÄƒ proxy pentru scraping.")
        client = httpx.AsyncClient(timeout=30)
        client.cookies.update(cookies)
        client.headers.update(headers)
        setattr(client, "proxy_url", None)
        sessions.append(client)

    return itertools.cycle(sessions)  # rotaÈ›ie (mÄƒcar 1 element)

async def run_orchestration():
    logger.info("ğŸš€ Pornim orchestratorul Reddit")
    logger.info(str(CONFIG_FILE))

    init_db()

    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    users = cfg.get("users", [])
    subreddits = cfg.get("subreddits", [])

    # 1) determinÄƒ un proxy funcÈ›ional (dacÄƒ existÄƒ) pentru login
    try:
        login_proxy = await get_next_working_proxy()
    except TypeError:
        # compatibil cu implementarea ta fÄƒrÄƒ parametri
        login_proxy = await get_next_working_proxy()

    if not login_proxy:
        logger.warning("âš ï¸ Niciun proxy funcÈ›ional â€” login fÄƒrÄƒ proxy")
        cookies, headers = await login_and_get_cookies(proxy=None)
    else:
        cookies, headers = await login_and_get_cookies(proxy=login_proxy)

    if not cookies:
        logger.error("âŒ Autentificarea a eÈ™uat. Oprire proces.")
        return

    # 2) creeazÄƒ rotaÈ›ia de sesiuni (cu fallback garantat fÄƒrÄƒ proxy)
    session_cycle = await create_sessions_with_proxies(cookies, headers)

    # 3) Users
    for user in users:
        session = next(session_cycle)
        proxy_label = getattr(session, "proxy_url", None)
        if proxy_label:
            logger.info(f"ğŸ“‹ [Scraping] Folosesc proxy: {proxy_label}")
        else:
            logger.info("ğŸ“‹ [Scraping] FÄƒrÄƒ proxy")
        user_data = await scrape_user_profile(user, session=session)
        if user_data:
            upsert_username_list([user_data])

    # 4) Subreddits
    for sub in subreddits:
        session = next(session_cycle)
        proxy_label = getattr(session, "proxy_url", None)
        if proxy_label:
            logger.info(f"ğŸ“‹ [Scraping] Folosesc proxy: {proxy_label}")
        else:
            logger.info("ğŸ“‹ [Scraping] FÄƒrÄƒ proxy")
        sub_data = await scrape_subreddit_about(sub, session=session)
        # if sub_data:
        #     upsert_subreddit_info(sub_data)

    logger.info("ğŸ Orchestrator finalizat cu succes")

if __name__ == "__main__":
    asyncio.run(run_orchestration())
