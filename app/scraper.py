# app/scraper.py

import asyncio
from app.services.auth_service import reddit_login   # FuncÈ›ie pentru autentificare pe Reddit
from app.services.parsing_service import get_recent_users  # FuncÈ›ie care extrage utilizatori recenÈ›i dintr-un subreddit
from app.services.storage_service import upsert_username_list  # FuncÈ›ie care salveazÄƒ/actualizeazÄƒ userii Ã®n DB
from app.utils.logger import logger  # Logger pentru mesaje structurate
from app.orchestration.orchestrator import create_sessions_with_proxies
# from settings import REDDIT_USER, REDDIT_PASS, HF_API_KEY
import time
import requests
from itertools import islice, cycle
import os
from dotenv import load_dotenv  # Pentru a Ã®ncÄƒrca variabilele din fiÈ™ierul .env

# ÃŽncarcÄƒ variabilele de mediu din fiÈ™ierul .env
load_dotenv()

# Citim user È™i parolÄƒ din .env (folosite la login pe Reddit)
USERNAME = os.getenv("REDDIT_USER")
PASSWORD = os.getenv("REDDIT_PASS")

# ------------------- FLUX PRINCIPAL ASINCRON -------------------
async def main():
    print("[INFO] Pornim procesul de login...")
    # Autentificare pe Reddit (returneazÄƒ True/False + sesiunea)
    logged_in, session = await reddit_login(USERNAME, PASSWORD)
    if not logged_in:
        print("[EROARE] Login eÈ™uat. Oprire.")
        return

    print("[OK] Login reuÈ™it! ÃŽncepem scraping utilizatori...")
    # Extragem utilizatori recenÈ›i din subreddit-ul AskReddit (max 50)
    users = await get_recent_users(session, subreddit="AskReddit", limit=50)
    print(f"[INFO] Am gÄƒsit {len(users)} utilizatori.")

    if users:
        # SalvÄƒm/actualizÄƒm lista de useri Ã®n baza de date
        upsert_username_list(users)  # Lista conÈ›ine dict-uri cu reddit_id + cÃ¢mpuri
        print("[OK] Lista de useri a fost salvatÄƒ Ã®n DB.")
    else:
        print("[WARN] Nu am gÄƒsit niciun user de salvat.")

# ------------------- SCRAPING FÄ‚RÄ‚ LOGIN -------------------
import requests
import time

BASE_URL = "https://www.reddit.com"
HEADERS = {"User-Agent": "Mozilla/5.0"}  # Header pentru a evita blocarea request-urilor

def collect_new_users(max_users=100):
    """
    ColecteazÄƒ utilizatori noi de pe Reddit folosind endpoint-ul public /new.json
    Nu necesitÄƒ autentificare.
    """
    users_data = {}
    after = None  # Folosit pentru paginare

    while len(users_data) < max_users:
        url = f"{BASE_URL}/new.json?limit=100"
        if after:
            url += f"&after={after}"
        r = requests.get(url, headers=HEADERS)
        r.raise_for_status()
        data = r.json()
        
        # IterÄƒm prin postÄƒrile returnate È™i extragem autorii
        for child in data["data"]["children"]:
            author = child["data"].get("author")
            if author and author not in users_data:
                users_data[author] = {
                    "reddit_id": child["data"].get("author_fullname"),
                    "name": author,
                    "is_online": False,       # Implicit offline
                    "recent_comments": []     # Va fi completat ulterior
                }
        
        # PregÄƒtim urmÄƒtoarea paginÄƒ
        after = data["data"].get("after")
        if not after:
            break

    return users_data

# ------------------- ÃŽMBOGÄ‚ÈšIREA DATELOR CU ACTIVITATE -------------------
async def enrich_with_activity(users_data, cookies, headers,
                               batch_size=10,
                               delay_between_requests=3,
                               delay_between_batches=10):
    """
    - Pentru fiecare user din users_data:
    - Ia ultimele 3 comentarii
    - SalveazÄƒ textul, subreddit-ul È™i timestamp-ul
    - MarcheazÄƒ userul ca online dacÄƒ a comentat Ã®n ultima orÄƒ
    - ProceseazÄƒ userii Ã®n loturi
    - FoloseÈ™te rotaÈ›ie de proxy-uri (o sesiune per batch)
    - Retry la 429 Too Many Requests
    - Pauze Ã®ntre request-uri È™i Ã®ntre batch-uri
    """
    now = time.time()
    usernames = list(users_data.keys())

    # CreeazÄƒ sesiunile cu proxy-uri
    session_list = await create_sessions_with_proxies(cookies, headers)
    session_cycle = cycle(session_list)  # pentru rotaÈ›ie

    def batched(iterable, n):
        it = iter(iterable)
        while batch := list(islice(it, n)):
            yield batch

    for batch in batched(usernames, batch_size):
        session = next(session_cycle)  # schimbÄƒ sesiunea la fiecare batch
        logger.info(f"ðŸ”„ Folosesc un nou proxy/session pentru acest batch ({len(batch)} useri)")

        for username in batch:
            url = f"{BASE_URL}/user/{username}/comments/.json?limit=3"
            logger.info(f"ðŸ“‹ enrich_with_activity {url}")

            try:
                r = await session.get(url, timeout=10)

                if r.status_code == 429:
                    logger.warning(f"[WARN] 429 Too Many Requests pentru {username}, aÈ™tept 120 secunde...")
                    time.sleep(120)
                    r = await session.get(url, timeout=120)

                r.raise_for_status()
                data = r.json()
                comments = data.get("data", {}).get("children", [])

                recent_comments = []
                is_online_flag = False

                for c in comments:
                    created = c["data"].get("created_utc", 0)
                    recent_comments.append({
                        "body": c["data"].get("body"),
                        "subreddit": c["data"].get("subreddit"),
                        "created_utc": created
                    })
                    # DacÄƒ a comentat Ã®n ultima orÄƒ â†’ considerÄƒm cÄƒ e online
                    if now - created < 3600:
                        is_online_flag = True

                # ActualizÄƒm datele userului
                users_data[username]["recent_comments"] = recent_comments
                users_data[username]["is_online"] = is_online_flag

            except requests.exceptions.RequestException as e:
                logger.warning(f"[WARN] Request failed for {username}: {e}")
            except ValueError as e:
                logger.warning(f"[WARN] JSON decode failed for {username}: {e}")
            except Exception as e:
                logger.error(f"[ERROR] Unexpected error for {username}: {e}")
                continue

            time.sleep(delay_between_requests)

        logger.info(f"â³ PauzÄƒ {delay_between_batches}s Ã®ntre batch-uri...")
        time.sleep(delay_between_batches)

    return users_data

# ------------------- PUNCT DE INTRARE -------------------
if __name__ == "__main__":
    asyncio.run(main())  # RuleazÄƒ fluxul asincron de login + scraping
