# =========================
# Importuri standard È™i din aplicaÈ›ie
# =========================
import asyncio          # pentru rularea funcÈ›iilor asincrone
import json             # pentru citirea fiÈ™ierului de configurare config.json
from pathlib import Path  # pentru lucrul cu cÄƒi de fiÈ™iere Ã®n mod portabil

# Servicii interne pentru login È™i scraping
from app.services.auth_service import reddit_login
from app.services.scrape_service import scrape_user_profile, scrape_subreddit_about

# Servicii interne pentru stocare Ã®n DB
from app.services.storage_service import upsert_username_list, upsert_subreddit_info

# Logger-ul aplicaÈ›iei
from app.utils.logger import logger

# IniÈ›ializare DB
from app.db.database import init_db

# Manager de proxy-uri
from app.utils.proxy_manager import get_next_working_proxy, proxies_list

import itertools        # pentru a crea un ciclu infinit de sesiuni
import httpx            # client HTTP asincron
import os
from dotenv import load_dotenv  # pentru Ã®ncÄƒrcarea variabilelor din .env
from colorama import Fore, Style  # pentru colorarea logurilor

# =========================
# ÃncÄƒrcare variabile de mediu
# =========================
load_dotenv()

# Directorul curent al fiÈ™ierului È™i calea cÄƒtre config.json
BASE_DIR = Path(__file__).resolve().parent
CONFIG_FILE = BASE_DIR / "config.json"

# Citirea credentialelor Reddit din .env
USERNAME = os.getenv("REDDIT_USER")
PASSWORD = os.getenv("REDDIT_PASS")

# =========================
# FuncÈ›ie: login_and_get_cookies
# =========================
async def login_and_get_cookies(proxy=None):
    """
    Face login pe Reddit È™i returneazÄƒ cookies + headers.
    DacÄƒ primeÈ™te un proxy, foloseÈ™te acel proxy pentru login.
    """
    if proxy:
        logger.info(f"âš ï¸ {Fore.GREEN}ğŸ“‹ Login folosind proxy: {proxy}{Style.RESET_ALL}")
        async with httpx.AsyncClient(proxy=proxy, timeout=30) as base_session:
            logged_in, _ = await reddit_login(USERNAME, PASSWORD, session=base_session)
            if not logged_in:
                return None, None
            cookies = base_session.cookies.jar
            headers = base_session.headers.copy()
        return cookies, headers
    else:
        logger.info(f"ğŸŒ {Fore.RED}Login fÄƒrÄƒ proxy{Style.RESET_ALL}")
        async with httpx.AsyncClient(timeout=30) as base_session:
            logged_in, _ = await reddit_login(USERNAME, PASSWORD, session=base_session)
            if not logged_in:
                return None, None
            cookies = base_session.cookies.jar
            headers = base_session.headers.copy()
        return cookies, headers

# =========================
# FuncÈ›ie: create_sessions_with_proxies
# =========================
async def create_sessions_with_proxies(cookies, headers):
    """
    CreeazÄƒ cÃ¢te o sesiune httpx pentru fiecare proxy funcÈ›ional.
    DacÄƒ niciun proxy nu e funcÈ›ional, creeazÄƒ o singurÄƒ sesiune fÄƒrÄƒ proxy.
    ReturneazÄƒ un itertools.cycle cu cel puÈ›in o sesiune (rotaÈ›ie infinitÄƒ).
    """
    sessions = []

    # TesteazÄƒ fiecare proxy din lista globalÄƒ
    for proxy in proxies_list:
        try:
            # ÃncearcÄƒ sÄƒ obÈ›inÄƒ un proxy funcÈ›ional pornind de la acesta
            working = await get_next_working_proxy(start_proxy=proxy)
        except TypeError:
            # DacÄƒ funcÈ›ia nu acceptÄƒ parametru, foloseÈ™te rotaÈ›ia internÄƒ
            working = await get_next_working_proxy()

        if not working:
            logger.warning(f"âš ï¸ Proxy {proxy} nu este funcÈ›ional, Ã®l sar.")
            continue

        # CreeazÄƒ client HTTPX cu proxy-ul funcÈ›ional
        client = httpx.AsyncClient(proxy=working, timeout=30)
        client.cookies.update(cookies)
        client.headers.update(headers)
        setattr(client, "proxy_url", working)  # salveazÄƒ proxy-ul Ã®n obiect pentru loguri
        sessions.append(client)
        logger.info(f"âœ… {Fore.GREEN}Sesiune adÄƒugatÄƒ pentru proxy: {working}{Style.RESET_ALL}")

    # DacÄƒ nu existÄƒ niciun proxy funcÈ›ional, foloseÈ™te conexiune directÄƒ
    if not sessions:
        logger.warning("âš ï¸ Niciun proxy funcÈ›ional â€” folosesc o sesiune fÄƒrÄƒ proxy pentru scraping.")
        client = httpx.AsyncClient(timeout=30)
        client.cookies.update(cookies)
        client.headers.update(headers)
        setattr(client, "proxy_url", None)
        sessions.append(client)
        logger.info(f"âœ… {Fore.GREEN}Sesiune adÄƒugatÄƒ fÄƒrÄƒ proxy{Style.RESET_ALL}")

    # ReturneazÄƒ un ciclu infinit de sesiuni (round-robin)
    return itertools.cycle(sessions)

# =========================
# FuncÈ›ia principalÄƒ: run_orchestration
# =========================
async def run_orchestration():
    """
    Orchestratorul principal:
    - IniÈ›ializeazÄƒ DB
    - CiteÈ™te config.json pentru lista de useri È™i subreddits
    - Face login pe Reddit (cu sau fÄƒrÄƒ proxy)
    - CreeazÄƒ sesiuni HTTPX (cu rotaÈ›ie de proxy-uri)
    - RuleazÄƒ scraping pentru fiecare user È™i subreddit
    - SalveazÄƒ datele Ã®n DB
    """
    logger.info("ğŸš€ Pornim orchestratorul Reddit")
    logger.info(str(CONFIG_FILE))

    # IniÈ›ializeazÄƒ baza de date
    init_db()

    # CiteÈ™te fiÈ™ierul de configurare
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    users = cfg.get("users", [])
    subreddits = cfg.get("subreddits", [])

    # 1) DeterminÄƒ un proxy funcÈ›ional pentru login
    try:
        login_proxy = await get_next_working_proxy()
    except TypeError:
        login_proxy = await get_next_working_proxy()

    # 2) Login cu sau fÄƒrÄƒ proxy
    if not login_proxy:
        logger.warning(f"âš ï¸ {Fore.RED}ğŸ“‹ Niciun proxy funcÈ›ional â€” login fÄƒrÄƒ proxy{Style.RESET_ALL}")
        cookies, headers = await login_and_get_cookies(proxy=None)
    else:
        logger.info(f"âš ï¸ {Fore.GREEN}ğŸ“‹ Avem proxy funcÈ›ional â€” {login_proxy}{Style.RESET_ALL}")
        cookies, headers = await login_and_get_cookies(proxy=login_proxy)

    if not cookies:
        logger.error("âŒ Autentificarea a eÈ™uat. Oprire proces.")
        return

    # 3) CreeazÄƒ rotaÈ›ia de sesiuni (cu fallback fÄƒrÄƒ proxy)
    session_cycle = await create_sessions_with_proxies(cookies, headers)

    # 4) Scraping pentru useri
    for user in users:
        session = next(session_cycle)
        proxy_label = getattr(session, "proxy_url", None)
        if proxy_label:
            logger.info(f"ğŸ“‹ {Fore.BLUE}[Scraping] Folosesc proxy: {proxy_label}{Style.RESET_ALL}")
        else:
            logger.info("ğŸ“‹ [Scraping] FÄƒrÄƒ proxy")
        user_data = await scrape_user_profile(user, session=session)
        if user_data:
            upsert_username_list([user_data])

    # 5) Scraping pentru subreddits
    for sub in subreddits:
        session = next(session_cycle)
        proxy_label = getattr(session, "proxy_url", None)
        if proxy_label:
            logger.info(f"ğŸ“‹ [Scraping] Folosesc proxy: {proxy_label}")
        else:
            logger.info("ğŸ“‹ [Scraping] FÄƒrÄƒ proxy")
        sub_data = await scrape_subreddit_about(sub, session=session)
        # DacÄƒ vrei sÄƒ salvezi info despre subreddit, decomenteazÄƒ linia de mai jos
        # if sub_data:
        #     upsert_subreddit_info(sub_data)

    logger.info("ğŸ Orchestrator finalizat cu succes")

# =========================
# RuleazÄƒ orchestratorul direct dacÄƒ fiÈ™ierul e executat standalone
# =========================
if __name__ == "__main__":
    asyncio.run(run_orchestration())
