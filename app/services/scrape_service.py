# =========================
# Importuri din aplicaÈ›ie
# =========================
from app.utils.logger import logger  # logger-ul centralizat al aplicaÈ›iei
from app.utils.http_client import fetch_with_retry  # funcÈ›ie utilitarÄƒ pentru request-uri HTTP cu retry automat

# =========================
# Constante
# =========================
BASE_URL = "https://www.reddit.com"  # URL-ul de bazÄƒ pentru API-ul public Reddit

# =========================
# FuncÈ›ie: scrape_user_profile
# =========================
async def scrape_user_profile(username: str, session):
    """
    Preia informaÈ›iile 'about' ale unui user Reddit.
    Parametri:
      - username: numele utilizatorului Reddit
      - session: obiect httpx.AsyncClient deja autentificat (cu cookie-uri/token)
    ReturneazÄƒ:
      - dict cu datele JSON ale profilului, sau None Ã®n caz de eroare
    """
    # VerificÄƒ dacÄƒ sesiunea este validÄƒ È™i are metoda .get
    if not session or not hasattr(session, "get"):
        logger.error(f"[SCRAPE USER] Sesiune invalidÄƒ pentru {username}, skip.")
        return None

    try:
        # ConstruieÈ™te URL-ul endpoint-ului JSON pentru profilul userului
        url = f"{BASE_URL}/user/{username}/about.json"
        # logger.info(f"ğŸ” Colectez date despre utilizator: {username}")

        # Face request GET cu retry automat
        resp = await fetch_with_retry("GET", url, session=session)
        # logger.info(f"âœ… Date preluate cu succes pentru {username} (status={resp.status_code})")

        # ReturneazÄƒ conÈ›inutul JSON
        return resp.json()

    except Exception as e:
        # LogheazÄƒ orice eroare apÄƒrutÄƒ Ã®n timpul scraping-ului
        logger.error(f"Eroare la scraping pentru {username}: {type(e).__name__} -> {e}")
        return None

# =========================
# FuncÈ›ie: scrape_subreddit_about
# =========================
async def scrape_subreddit_about(subreddit: str, session):
    """
    Preia informaÈ›iile 'about' ale unui subreddit.
    Parametri:
      - subreddit: numele subreddit-ului (fÄƒrÄƒ prefixul r/)
      - session: obiect httpx.AsyncClient deja autentificat
    ReturneazÄƒ:
      - dict cu datele JSON ale subreddit-ului, sau None Ã®n caz de eroare
    """
    # VerificÄƒ dacÄƒ sesiunea este validÄƒ
    if not session or not hasattr(session, "get"):
        logger.error(f"[SCRAPE SUB] Sesiune invalidÄƒ pentru r/{subreddit}, skip.")
        return None

    try:
        # ConstruieÈ™te URL-ul endpoint-ului JSON pentru pagina "about" a subreddit-ului
        url = f"{BASE_URL}/r/{subreddit}/about.json"
        # logger.info(f"ğŸ” Colectez date despre subreddit: {subreddit}")

        # Face request GET cu retry automat
        resp = await fetch_with_retry("GET", url, session=session)
        # logger.info(f"âœ… Date preluate cu succes pentru r/{subreddit} (status={resp.status_code})")

        # ReturneazÄƒ conÈ›inutul JSON
        return resp.json()

    except Exception as e:
        # LogheazÄƒ eroarea È™i returneazÄƒ None
        logger.error(f"Eroare la scraping pentru r/{subreddit}: {type(e).__name__} -> {e}")
        return None
